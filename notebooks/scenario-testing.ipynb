{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook\n",
    "Generated from script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n\n",
    "from pokemon_predictor import config\n",
    "from pokemon_predictor.data_utils import load_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Scenario Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokemon_predictor.predict import PokemonPredictor\n",
    "from pokemon_predictor import config\n",
    "import pandas as pd\n\n",
    "predictor = PokemonPredictor()\n\n",
    "scenarios = {\n",
    "    \"Mono-Type\": [\"Charmander\", \"Squirtle\"],\n",
    "    \"Dual-Type\": [\"Charizard\", \"Gengar\"],\n",
    "    \"Impostors\": [\"Sudowoodo\", \"Groudon\"]\n",
    "}\n\n",
    "print(f\"{'Pokemon':<15} | {'True Type':<20} | {'XGBoost':<20} | {'MLP':<20}\")\n",
    "print(\"-\" * 80)\n\n",
    "df_meta = pd.read_csv(config.PROCESSED_DATA_DIR / \"pokemon_metadata.csv\")\n\n",
    "for cat, names in scenarios.items():\n",
    "    print(f\"--- {cat} ---\")\n",
    "    for name in names:\n",
    "        row = df_meta[df_meta['name'].str.lower() == name.lower()]\n",
    "        if row.empty: continue\n",
    "        img_path = config.RAW_DATA_DIR / f\"{row.iloc[0]['name']}.png\"\n",
    "        pred = predictor.predict(str(img_path))\n",
    "        t1 = row.iloc[0]['type1']\n",
    "        t2 = row.iloc[0]['type2']\n",
    "        true_t = f\"{t1}\" + (f\", {t2}\" if pd.notna(t2) else \"\")\n",
    "        if pred:\n",
    "            xgb = \", \".join(pred['xgboost'])\n",
    "            mlp = \", \".join(pred['mlp'])\n",
    "            print(f\"{name:<15} | {true_t:<20} | {xgb:<20} | {mlp:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Penalization Grid Search\n",
    "Testing different colsample_bytree values to find the optimal regularization against Biological Ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score, precision_score\n\n",
    "X_train, X_test, y_train, y_test, _ = load_data('hybrid', split_data=True)\n\n",
    "penalties = [1.0, 0.75, 0.50, 0.25]\n",
    "results = []\n\n",
    "for penalty in penalties:\n",
    "    model = MultiOutputClassifier(XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, colsample_bytree=penalty, n_jobs=-1, random_state=config.RANDOM_SEED))\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_probs = model.predict_proba(X_test)\n",
    "    preds = np.array([p[:, 1] for p in pred_probs]).T > 0.5\n",
    "    f1 = f1_score(y_test, preds, average='micro')\n",
    "    prec = precision_score(y_test, preds, average='micro')\n",
    "    results.append({'Penalty': penalty, 'F1 Micro': f1, 'Precision': prec})\n\n",
    "df_res = pd.DataFrame(results).sort_values(by='F1 Micro', ascending=False)\n",
    "print(\"\\n=== Scenario Ranking ===\")\n",
    "display(df_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}