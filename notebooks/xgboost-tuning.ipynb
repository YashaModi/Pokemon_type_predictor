{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7c35b8",
   "metadata": {},
   "source": [
    "# Advanced XGBoost Tuning (Bayesian + SMOTE)\n",
    "Implementing Optuna for hyperparameter optimization and SMOTE to handle minority class imbalances (Ghost, Ice, Fairy, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6eb5c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:07:27.180827Z",
     "iopub.status.busy": "2026-02-21T14:07:27.180616Z",
     "iopub.status.idle": "2026-02-21T14:07:32.596921Z",
     "shell.execute_reply": "2026-02-21T14:07:32.596605Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from pokemon_predictor import config\n",
    "from pokemon_predictor.data_utils import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ff107f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:07:32.598663Z",
     "iopub.status.busy": "2026-02-21T14:07:32.598429Z",
     "iopub.status.idle": "2026-02-21T14:07:32.717229Z",
     "shell.execute_reply": "2026-02-21T14:07:32.716707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Hybrid + Bio-Ratio Dataset\n",
    "X_train, X_test, y_train, y_test, classes = load_data('hybrid', split_data=True)\n",
    "\n",
    "X_train_np = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "y_train_np = y_train.values if isinstance(y_train, pd.DataFrame) else y_train\n",
    "X_test_np = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "y_test_np = y_test.values if isinstance(y_test, pd.DataFrame) else y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308edd7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:07:32.718307Z",
     "iopub.status.busy": "2026-02-21T14:07:32.718242Z",
     "iopub.status.idle": "2026-02-21T14:07:32.721236Z",
     "shell.execute_reply": "2026-02-21T14:07:32.720945Z"
    }
   },
   "outputs": [],
   "source": [
    "class SMOTEMultiOutputClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Custom wrapper that applies SMOTE independently to each label before fitting its respective XGBoost classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, random_state=None):\n",
    "        self.estimator = estimator\n",
    "        self.random_state = random_state\n",
    "        self.estimators_ = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimators_ = []\n",
    "        for i in range(y.shape[1]):\n",
    "            y_i = y[:, i]\n",
    "            n_minority = np.sum(y_i)\n",
    "            \n",
    "            if 0 < n_minority < len(y_i):\n",
    "                # Adjust k_neighbors bounded by minority size\n",
    "                k_neighbors = min(5, int(n_minority) - 1)\n",
    "                smote = SMOTE(k_neighbors=max(1, k_neighbors), random_state=self.random_state)\n",
    "                try:\n",
    "                    X_res, y_res = smote.fit_resample(X, y_i)\n",
    "                except ValueError:\n",
    "                    X_res, y_res = X, y_i\n",
    "            else:\n",
    "                X_res, y_res = X, y_i\n",
    "                \n",
    "            est = clone(self.estimator)\n",
    "            est.fit(X_res, y_res)\n",
    "            self.estimators_.append(est)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = [est.predict(X) for est in self.estimators_]\n",
    "        return np.column_stack(preds)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return [est.predict_proba(X) for est in self.estimators_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d534430d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:07:32.722218Z",
     "iopub.status.busy": "2026-02-21T14:07:32.722166Z",
     "iopub.status.idle": "2026-02-21T14:32:17.829586Z",
     "shell.execute_reply": "2026-02-21T14:32:17.828883Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-21 16:07:32,723]\u001b[0m A new study created in memory with name: no-name-3840456c-dd2b-417d-8353-8a54de1e8097\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optuna Best Parameters ===\n",
      "{'max_depth': 9, 'learning_rate': 0.28923071630444935, 'gamma': 1.2663559946940675e-05, 'min_child_weight': 7, 'subsample': 0.9937843996822882, 'colsample_bytree': 0.9433842385679996}\n",
      "Best Cross-Validation F1: 0.3594\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 9)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 7)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.4, 1.0)\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=config.RANDOM_SEED)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train_np):\n",
    "        X_tr, X_val = X_train_np[train_idx], X_train_np[val_idx]\n",
    "        y_tr, y_val = y_train_np[train_idx], y_train_np[val_idx]\n",
    "        \n",
    "        base_est = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            gamma=gamma,\n",
    "            min_child_weight=min_child_weight,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            n_jobs=-1,\n",
    "            random_state=config.RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        clf = SMOTEMultiOutputClassifier(base_est, random_state=config.RANDOM_SEED)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        \n",
    "        preds = clf.predict(X_val)\n",
    "        score = f1_score(y_val, preds, average='micro')\n",
    "        scores.append(score)\n",
    "        \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=30)  # 30 trials for demonstration speed\n",
    "\n",
    "print(\"=== Optuna Best Parameters ===\")\n",
    "print(study.best_params)\n",
    "print(f\"Best Cross-Validation F1: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1e3138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T14:32:17.832335Z",
     "iopub.status.busy": "2026-02-21T14:32:17.832218Z",
     "iopub.status.idle": "2026-02-21T14:32:49.457277Z",
     "shell.execute_reply": "2026-02-21T14:32:49.456807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final optimized SMOTE model on full training set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TEST F1 Micro Score (Optuna + SMOTE): 0.4254\n",
      "Saved optimized model to /Users/yasha/Desktop/projects/pokemon_type_predictor/models/xgboost_optuna_smote.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Training final optimized SMOTE model on full training set...\")\n",
    "best_est = XGBClassifier(\n",
    "    n_estimators=200, # Increased for final fit\n",
    "    **study.best_params,\n",
    "    n_jobs=-1,\n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "final_clf = SMOTEMultiOutputClassifier(best_est, random_state=config.RANDOM_SEED)\n",
    "final_clf.fit(X_train_np, y_train_np)\n",
    "\n",
    "test_preds = final_clf.predict(X_test_np)\n",
    "final_f1 = f1_score(y_test_np, test_preds, average='micro')\n",
    "print(f\"\\nFINAL TEST F1 Micro Score (Optuna + SMOTE): {final_f1:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "out_path = config.MODELS_DIR / \"xgboost_optuna_smote.pkl\"\n",
    "joblib.dump(final_clf, out_path)\n",
    "print(f\"Saved optimized model to {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
